############################ Activation Function #####################################

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

def plot_linear():
    x = np.linspace(-10, 10, 100)
    y = x
    
    plt.plot(x, y)
    plt.xlabel('Input')
    plt.ylabel('Linear Output')
    plt.title('Linear Activation Function')
    plt.grid(True)
    plt.show()
    
plot_linear()

def plot_sigmoid():
    x = np.linspace(-10, 10, 100)
    y = 1/(1 + np.exp(-x))
    
    plt.plot(x, y)
    plt.xlabel('Input')
    plt.ylabel('Sigmoid Output')
    plt.title('Sigmoid Activation Function')
    plt.grid(True)
    plt.show()
    
plot_sigmoid()

def plot_tanh():
    x = np.linspace(-10,10,100)
    y = np.tanh(x)
    
    plt.plot(x,y)
    plt.xlabel('Input')
    plt.ylabel('tanh Output')
    plt.title('Hyperbolic Tangent Activation Function')
    plt.grid(True)
    plt.show()
    
plot_tanh()

def plot_relu():
    x = np.linspace(-10,10,100)
    y = np.maximum(0,x)
    
    plt.plot(x,y)
    plt.xlabel('Input')
    plt.ylabel('Relu Output')
    plt.title('relu Activation Fucntion')
    plt.grid()
    plt.show()
    
plot_relu()






######################## McCullochPitts #####################################

import numpy as np
import matplotlib.pyplot as plt

class McCullochPitts:
    def __init__(self, input_size):
        self.weights = np.zeros(input_size)
        self.bais = 0
        
    def predict(self, inputs):
        linear_combination = np.dot(self.weights, inputs) + self.bais
        return 1 if linear_combination >= 0 else 0
    
    

inputs = np.array([[0,0], [0,1], [1,0], [1,1]])
expected_outputs = np.array([0, 0, 1, 0])

neural_network = McCullochPitts(2)

learning_rate = 0.1
epochs = 10
for epoch in range(epochs):
    for input, expected_output in zip(inputs, expected_outputs):
        prediction  = neural_network.predict(input)
        error = expected_output - prediction
        neural_network.weights += learning_rate * error * input
        neural_network.bais +=learning_rate * error
        
for input in inputs:
    print(f'Inputs : {input}')
    if neural_network.predict(input) == 1:
        print('ANDNOT output : True')
    else:
        print('ANDNOT output : False')




############################ ASCII ###########################################

import numpy as np
import pandas as pd

training_data = np.array([[48, 0],
                        [49, 1], 
                        [50, 0],
                        [51, 1],
                        [52, 0],
                        [53, 1], 
                        [54, 0],
                        [55, 1],
                        [56, 0],
                        [57, 1]])

x = training_data[:,0].reshape(-1,1)
y = training_data[:,1]

perceptron = np.where(np.sum(x, axis=1) % 2 == 0, 0, 1)

test_data = np.array([48, 49, 50, 51, 52, 53, 54, 55, 56, 57])
predictions = np.where(np.sum(test_data.reshape(-1,1), axis=1) % 2 == 0, 0, 1)

for ascii_val, prediction in zip(test_data, predictions):
    number = chr(ascii_val)
    parity = 'even' if prediction == 0 else 'odd'
    print(f"Number : {number}, ASCII value : {ascii_val}, Output : {parity}")



########################### Perceptron Learning Law ########################################

import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt

class Perceptron:
    def __init__(self, lr=0.1, n_iter=100):
        self.lr = lr
        self.n_iter = n_iter
        self.weights = None
        self.bais = None
        
    def fit(self, x, y):
        self.weights = np.zeros(x.shape[1])
        self.bais = 1
        
        for i in range(self.n_iter):
            for j in range(x.shape[0]):
                pred = self.predict(x[j])
                
                if pred != y[j]:
                    self.weights += self.lr * y[j] *x[j]
                    self.bais += self.lr * y[j]
                    
    def predict(self, x):
        output = np.dot(x, self.weights) + self.bais
        return np.where(output >= 0, 1, -1)

x = np.array([[2,2], [4,4], [4,0], [3,2], [8,4], [8,0]])
y = np.array([1, 1, -1, 1, -1, -1])

model = Perceptron()
model.fit(x, y)

plt.scatter(x[:, 0], x[:, 1], c=y)
plt.xlim(0, 10)
plt.ylim(0, 5)
x1 = np.linspace(0, 10)
x2 = -(model.weights[0]*x1 + model.bais)/model.weights[1]
plt.plot(x1, x2, '-r')
plt.show()



############################ BAM ###############################

import numpy as np

# Define two sets of patterns: Set A and Set B
# Set A: Input Patterns
x1 = np.array([1, 1, 1, 1, 1, 1]).reshape(6, 1)
x2 = np.array([-1, -1, -1, -1, -1, -1]).reshape(6, 1)

# Set B: Target Patterns
y1 = np.array([1, 1]).reshape(2, 1)
y2 = np.array([-1, -1]).reshape(2, 1)

# Calculate the weight matrices: W and W_T
W = np.dot(x1, y1.T) + np.dot(x2, y2.T)
W_T = np.dot(y1, x1.T) + np.dot(y2, x2.T)

# Testing Phase
# Test for Input Patterns: Set A
print("Testing for input patterns: Set A")
def testInputs(x, weight):
    # Compute the output pattern
    y = np.dot(weight, x)
    y[y < 0] = -1
    y[y >= 0] = 1
    return y

print("\nOutput of input pattern 1:")
print(testInputs(x1, W_T))
print("\nOutput of input pattern 2:")
print(testInputs(x2, W_T))

# Test for Target Patterns: Set B
print("\nTesting for target patterns: Set B")
def testTargets(y, weight):
    # Compute the output pattern
    x = np.dot(weight, y)
    x[x <= 0] = -1
    x[x > 0] = 1
    return x

print("\nOutput of target pattern 1:")
print(testTargets(y1, W))
print("\nOutput of target pattern 2:")
print(testTargets(y2, W))




############################## ART #######################################

import numpy as np
import pandas as pd

class ART:
    def __init__(self, input_size, rho, alpha):
        self.w = np.ones((1, input_size))
        self.rho = rho
        self.alpha = alpha
    
    def reset(self):
        self.w = np.ones((1, input_size))
        
    def train(self, x):
        while True:
            y = self.predict(x)
            if y is not None:
                self.update(x)
                return
            else:
                self.reset()
                
    def predict(self, x):
        y = x.dot(self.w.T)
        if y >= self.rho:
            return y
        else:
            return None
        
    def update(self, x):
        self.w = self.alpha * x + (1 - self.alpha) * self.w
        

if __name__ == '__main__':
    input_size = 2
    rho = 0.9
    alpha = 0.1
    network = ART(input_size, rho, alpha)
    x1 = np.array([0.7, 0.3])
    x2 = np.array([0.2, 0.8])
    x3 = np.array([0.6, 0.6])
    network.train(x1)
    network.train(x2)
    network.train(x3)
    print(network.w)




######################### Hopfield Network ###############################

import numpy as np

v1 = np.array([1, -1, 1, -1])
v2 = np.array([-1, 1, -1, 1])
v3 = np.array([1, 1, -1, -1])
v4 = np.array([-1, -1, 1, 1])

w = np.outer(v1, v1) + np.outer(v2, v2) + np.outer(v3, v3) + np.outer(v4, v4)

np.fill_diagonal(w, 0)

def activation(x):
    return np.where(x >= 0, 1, -1)

def hopfield_network(input_vector, weight_matrix, activation_function, iteration):
    state  = input_vector
    for i in range (iteration):
        net_input = np.dot(weight_matrix, state)
        new_state = activation_function(net_input)
        if np.array_equal(new_state,state):
            return new_state
        state = new_state
    return state

input_vector = np.array([-1, -1, 1, -1])
output_vector = hopfield_network(input_vector, w, activation, 10)
print('Input vector', input_vector)
print('Output vector', output_vector)
w


########################## Logistic Regression ######################################

import tensorflow as tf
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_breast_cancer
from tensorflow.keras.layers import Dense
from tensorflow.keras import Sequential

df = load_breast_cancer()

x_train, x_test, y_train, y_test = train_test_split(df.data, df.target, test_size=0.20, random_state=42)

sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

model = Sequential()
model.add(Dense(128,activation='relu' ,  input_shape = (30,)))
model.add(Dense(1,activation='sigmoid'))

model.summary()

model = tf.keras.models.Sequential([tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(x_train.shape[1],))])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)

y_pred = model.predict(x_test)

test_loss, test_accuracy = model.evaluate(x_test, y_test)

print('Accuracy is : ', test_accuracy)

print('Loss is : ', test_loss)





################################ CNN ###################################

import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.utils import to_categorical

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train.reshape(-1, 28, 28, 1)/ 255.0
x_test = x_test.reshape(-1, 28, 28, 1) / 255.0
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

model = Sequential([
    Conv2D(32, (3, 3), activation = 'relu', input_shape = (28, 28, 1)),
    MaxPooling2D((2,2)),
    Conv2D(64, (3,3), activation = 'relu'),
    MaxPooling2D((2,2)),
    Conv2D(64, (3,3), activation = 'relu'),
    Flatten(),
    Dense(64, activation = 'relu'),
    Dense(10, activation = 'softmax')
])

model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])

model.fit(x_train, y_train, batch_size = 64, epochs = 10, verbose = 1)

loss, accuracy = model.evaluate(x_test, y_test)
print(f'Test Loss : {loss}')
print(f'Test Accuracy : {accuracy}')


####################### MNIST ###########################

import tensorflow as trf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam

#Load and preprocess the MNIST dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train = X_train / 255.0
X_test = X_test / 255.0

# Define the model architecture
model = Sequential([Flatten(input_shape = (28, 28)),
                   Dense(128, activation='relu'),
                   Dense(10, activation='softmax')])

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001),
             loss='sparse_categorical_crossentropy',
             metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, batch_size=64, epochs=10, verbose=1)

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {loss}")
print(f"Test Accuracy: {accuracy}")

